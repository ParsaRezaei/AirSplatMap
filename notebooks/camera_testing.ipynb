{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54dc5534",
   "metadata": {},
   "source": [
    "# AirSplatMap Camera Testing\n",
    "\n",
    "Test camera connection and pose/depth estimation in real-time.\n",
    "\n",
    "## Features\n",
    "- Camera connection testing\n",
    "- Live pose estimation with trajectory visualization\n",
    "- Depth estimation preview\n",
    "- Compare multiple pose estimators side-by-side\n",
    "\n",
    "## Requirements\n",
    "- Webcam or RealSense camera\n",
    "- OpenCV (`pip install opencv-python`)\n",
    "- matplotlib for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "\n",
    "# Add project root\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5870ff",
   "metadata": {},
   "source": [
    "## 1. Test Camera Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_camera(camera_id=0):\n",
    "    \"\"\"Test if camera is accessible.\"\"\"\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"❌ Cannot open camera {camera_id}\")\n",
    "        return False\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        print(f\"✅ Camera {camera_id} working! Frame size: {frame.shape}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"❌ Camera {camera_id} cannot capture frames\")\n",
    "        return False\n",
    "\n",
    "# Test default camera\n",
    "test_camera(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_single_frame(camera_id=0):\n",
    "    \"\"\"Capture and display a single frame.\"\"\"\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        # Convert BGR to RGB for display\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Camera {camera_id} - {frame.shape[1]}x{frame.shape[0]}')\n",
    "        plt.show()\n",
    "        return frame_rgb\n",
    "    else:\n",
    "        print(\"Failed to capture frame\")\n",
    "        return None\n",
    "\n",
    "frame = capture_single_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27823d3",
   "metadata": {},
   "source": [
    "## 2. Test Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97705639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pose import get_pose_estimator, list_pose_estimators\n",
    "\n",
    "print(\"Available pose estimators:\")\n",
    "for name, info in list_pose_estimators().items():\n",
    "    gpu = \"(GPU)\" if info.get('gpu') else \"\"\n",
    "    print(f\"  - {name}: {info['description']} {gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce004ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pose estimator\n",
    "POSE_METHOD = 'orb'  # Change to 'sift', 'loftr', 'superpoint' etc.\n",
    "\n",
    "pose_estimator = get_pose_estimator(POSE_METHOD)\n",
    "\n",
    "# Set camera intrinsics (adjust for your camera)\n",
    "# Default webcam approximation for 640x480\n",
    "pose_estimator.set_intrinsics_from_dict({\n",
    "    'fx': 525.0,\n",
    "    'fy': 525.0,\n",
    "    'cx': 320.0,\n",
    "    'cy': 240.0,\n",
    "})\n",
    "\n",
    "print(f\"Initialized {POSE_METHOD} pose estimator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80bbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pose_on_frames(n_frames=30, camera_id=0):\n",
    "    \"\"\"Capture frames and test pose estimation.\"\"\"\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    \n",
    "    positions = []\n",
    "    fps_list = []\n",
    "    \n",
    "    print(f\"Capturing {n_frames} frames...\")\n",
    "    print(\"Move the camera slowly to test tracking.\")\n",
    "    \n",
    "    for i in range(n_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert to RGB\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Estimate pose\n",
    "        t0 = time.time()\n",
    "        result = pose_estimator.estimate(rgb)\n",
    "        dt = time.time() - t0\n",
    "        \n",
    "        positions.append(result.pose[:3, 3].copy())\n",
    "        fps_list.append(1.0 / dt if dt > 0 else 0)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            pos = result.pose[:3, 3]\n",
    "            print(f\"Frame {i+1}: pos=[{pos[0]:.3f}, {pos[1]:.3f}, {pos[2]:.3f}], \"\n",
    "                  f\"inliers={result.num_inliers}, FPS={fps_list[-1]:.1f}\")\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    return np.array(positions), fps_list\n",
    "\n",
    "positions, fps_list = test_pose_on_frames(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trajectory\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(positions) > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Trajectory (XZ plane)\n",
    "    axes[0].plot(positions[:, 0], positions[:, 2], 'b-o', markersize=3)\n",
    "    axes[0].plot(positions[0, 0], positions[0, 2], 'go', markersize=10, label='Start')\n",
    "    axes[0].plot(positions[-1, 0], positions[-1, 2], 'ro', markersize=10, label='End')\n",
    "    axes[0].set_xlabel('X (m)')\n",
    "    axes[0].set_ylabel('Z (m)')\n",
    "    axes[0].set_title('Camera Trajectory (Top-Down)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    axes[0].axis('equal')\n",
    "    \n",
    "    # FPS over time\n",
    "    axes[1].plot(fps_list, 'g-')\n",
    "    axes[1].axhline(y=np.mean(fps_list), color='r', linestyle='--', label=f'Avg: {np.mean(fps_list):.1f}')\n",
    "    axes[1].set_xlabel('Frame')\n",
    "    axes[1].set_ylabel('FPS')\n",
    "    axes[1].set_title(f'Processing Speed ({POSE_METHOD})')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough frames captured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8475c",
   "metadata": {},
   "source": [
    "## 3. Test Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3840eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test depth estimation on a single frame\n",
    "try:\n",
    "    from benchmarks.depth.benchmark_depth import get_depth_estimator, list_depth_estimators\n",
    "    \n",
    "    print(\"Available depth estimators:\")\n",
    "    for name, info in list_depth_estimators().items():\n",
    "        print(f\"  - {name}: {info['description']}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Depth estimators not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1893d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_METHOD = 'midas_small'  # Fast option\n",
    "\n",
    "try:\n",
    "    depth_estimator = get_depth_estimator(DEPTH_METHOD)\n",
    "    print(f\"Initialized {DEPTH_METHOD} depth estimator\")\n",
    "    \n",
    "    # Capture and process\n",
    "    if frame is not None:\n",
    "        print(\"Estimating depth...\")\n",
    "        t0 = time.time()\n",
    "        depth = depth_estimator.estimate(frame)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"Done in {dt:.2f}s ({1/dt:.1f} FPS)\")\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        axes[0].imshow(frame)\n",
    "        axes[0].set_title('RGB Input')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        im = axes[1].imshow(depth, cmap='plasma')\n",
    "        axes[1].set_title(f'Estimated Depth ({DEPTH_METHOD})')\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Depth estimation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c5a6c",
   "metadata": {},
   "source": [
    "## 4. Compare Pose Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f16290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pose_estimators(methods=['orb', 'sift', 'robust_flow'], n_frames=20, camera_id=0):\n",
    "    \"\"\"Compare multiple pose estimators on live camera.\"\"\"\n",
    "    \n",
    "    # Capture frames first\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    frames = []\n",
    "    \n",
    "    print(f\"Capturing {n_frames} frames...\")\n",
    "    for i in range(n_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        time.sleep(0.05)  # ~20 FPS capture\n",
    "    cap.release()\n",
    "    \n",
    "    print(f\"Captured {len(frames)} frames\")\n",
    "    \n",
    "    # Test each method\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\nTesting {method}...\")\n",
    "        try:\n",
    "            estimator = get_pose_estimator(method)\n",
    "            estimator.set_intrinsics_from_dict({\n",
    "                'fx': 525.0, 'fy': 525.0, 'cx': 320.0, 'cy': 240.0\n",
    "            })\n",
    "            \n",
    "            positions = []\n",
    "            times = []\n",
    "            \n",
    "            for frame in frames:\n",
    "                t0 = time.time()\n",
    "                result = estimator.estimate(frame)\n",
    "                times.append(time.time() - t0)\n",
    "                positions.append(result.pose[:3, 3].copy())\n",
    "            \n",
    "            results[method] = {\n",
    "                'positions': np.array(positions),\n",
    "                'avg_fps': 1.0 / np.mean(times),\n",
    "                'times': times,\n",
    "            }\n",
    "            print(f\"  {method}: Avg FPS = {results[method]['avg_fps']:.1f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  {method}: FAILED - {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "comparison = compare_pose_estimators(['orb', 'sift', 'robust_flow'], n_frames=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "if comparison:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Trajectories\n",
    "    for method, data in comparison.items():\n",
    "        pos = data['positions']\n",
    "        axes[0].plot(pos[:, 0], pos[:, 2], '-o', markersize=2, label=method)\n",
    "    \n",
    "    axes[0].set_xlabel('X (m)')\n",
    "    axes[0].set_ylabel('Z (m)')\n",
    "    axes[0].set_title('Trajectory Comparison')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # FPS comparison\n",
    "    methods = list(comparison.keys())\n",
    "    fps_values = [comparison[m]['avg_fps'] for m in methods]\n",
    "    \n",
    "    axes[1].bar(methods, fps_values, color=['#1f77b4', '#ff7f0e', '#2ca02c'][:len(methods)])\n",
    "    axes[1].set_ylabel('FPS')\n",
    "    axes[1].set_title('Processing Speed Comparison')\n",
    "    \n",
    "    for i, v in enumerate(fps_values):\n",
    "        axes[1].text(i, v + 1, f'{v:.1f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a16de7",
   "metadata": {},
   "source": [
    "## 5. Interactive Live View (Optional)\n",
    "\n",
    "For continuous live streaming, it's better to use the web dashboard:\n",
    "\n",
    "```bash\n",
    "# Start RealSense server\n",
    "python scripts/tools/realsense_server.py --port 8554\n",
    "\n",
    "# Start dashboard\n",
    "python scripts/web_dashboard.py\n",
    "```\n",
    "\n",
    "Then open http://localhost:9002 in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31948cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To run live 3DGS with camera:\")\n",
    "print(\"\")\n",
    "print(\"1. Start RealSense server:\")\n",
    "print(\"   python scripts/tools/realsense_server.py --port 8554\")\n",
    "print(\"\")\n",
    "print(\"2. Start dashboard:\")\n",
    "print(\"   python scripts/web_dashboard.py\")\n",
    "print(\"\")\n",
    "print(\"3. Open browser:\")\n",
    "print(\"   http://localhost:9002\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
